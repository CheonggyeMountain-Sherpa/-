{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d99e7ae6-1680-4d09-b102-907c9a117cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_relationship_path = 'data/causal_relationship_inference'\n",
    "homograph_path = 'data/homograph'\n",
    "interrogation_globale_path = 'data/interrogation_globale'\n",
    "sentence_grammaticality_judgment_path = 'data/sentence_grammaticality_judgment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4266e8b9-5943-4fb5-b157-a524c7e74dbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "origin_copa_train_csv = pd.read_csv(os.path.join(causal_relationship_path, 'SKT_COPA_Train.tsv'), delimiter='\\t')\n",
    "origin_copa_dev_csv = pd.read_csv(os.path.join(causal_relationship_path, 'SKT_COPA_Dev.tsv'), delimiter='\\t')\n",
    "origin_copa_test_csv = pd.read_csv(os.path.join(causal_relationship_path, 'SKT_COPA_Test.tsv'), delimiter='\\t')\n",
    "\n",
    "# origin_nikl_train_csv = pd.read_csv(os.path.join(homograph_path, 'NIKL_SKT_WiC_Train.txv'), delimiter='\\t')\n",
    "# origin_nikl_dev_csv = pd.read_csv(os.path.join(homograph_path, 'NIKL_SKT_WiC_Dev.txv'), delimiter='\\t')\n",
    "# origin_nikl_test_csv = pd.read_csv(os.path.join(homograph_path, 'NIKL_SKT_WiC_Test.txv'), delimiter='\\t')\n",
    "\n",
    "# origin_boolq_train_csv = pd.read_csv(os.path.join(interrogation_globale_path, 'SKT_BoolQ_Train.tsv'), delimiter='\\t')\n",
    "# origin_boolq_dev_csv = pd.read_csv(os.path.join(interrogation_globale_path, 'SKT_BoolQ_Dev.tsv'), delimiter='\\t')\n",
    "# origin_boolq_test_csv = pd.read_csv(os.path.join(interrogation_globale_path, 'SKT_BoolQ_Test.tsv'), delimiter='\\t')\n",
    "\n",
    "# origin_cola_train_csv = pd.read_csv(os.path.join(interrogation_globale_path, 'NIKL_CoLA_train.tsv'), delimiter='\\t')\n",
    "# origin_cola_dev_csv = pd.read_csv(os.path.join(interrogation_globale_path, 'NIKL_CoLA_dev.tsv'), delimiter='\\t')\n",
    "# origin_cola_test_csv = pd.read_csv(os.path.join(interrogation_globale_path, 'NIKL_CoLA_test.tsv'), delimiter='\\t')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c111f17-5bfc-4ebf-8183-fa02b55969e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import GPT2TokenizerFast, GPT2Config, GPT2ForSequenceClassification\n",
    "from transformers import AutoTokenizer, AdamW\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6428b608-9960-4be8-8049-f6d9c97a5981",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "# Custom IPython progress bar for training\n",
    "class ProgressMonitor(object):\n",
    "    \n",
    "    tmpl = \"\"\"\n",
    "        <table style=\"width: 100%;\">\n",
    "            <tbody>\n",
    "                <tr>\n",
    "                    <td style=\"width: 30%;\">\n",
    "                     <b>Epoch: {epoch}/{num_epochs} Loss: {loss:0.4f}</b> &nbsp&nbsp&nbsp {value} / {length}\n",
    "                    </td>\n",
    "                    <td style=\"width: 70%;\">\n",
    "                        <progress value='{value}' max='{length}', style='width: 100%'>{value}</progress>\n",
    "                    </td>\n",
    "                </tr>\n",
    "            </tbody>\n",
    "        </table>        \n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, length):\n",
    "        self.length = length\n",
    "        self.count = 0\n",
    "        self.display = display(self.html(0, 0, 0, 0), display_id=True)\n",
    "        \n",
    "    def html(self, count, loss, epoch, num_epochs):\n",
    "        return HTML(self.tmpl.format(length=self.length, value=count, loss=loss, epoch=epoch, num_epochs=num_epochs))\n",
    "        \n",
    "    def update(self, epoch, num_epochs, count, loss):\n",
    "        self.count += count\n",
    "        self.display.update(self.html(self.count, loss, epoch, num_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a23f8c67-de43-4c61-a90b-806a5d7ab91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "num_labels = 2\n",
    "batch_size = 16\n",
    "learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efcaedc4-10df-491d-8fdf-8362653dd499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['결과' '원인' '원인 ' '결과 ']\n",
      "[1 2]\n",
      "['결과' '원인']\n",
      "[2 1]\n",
      "['결과' '원인']\n",
      "[nan]\n"
     ]
    }
   ],
   "source": [
    "causal_relationship_path = 'data/causal_relationship_inference'\n",
    "origin_copa_train_csv = pd.read_csv(os.path.join(causal_relationship_path, 'SKT_COPA_Train.tsv'), delimiter='\\t')\n",
    "origin_copa_dev_csv = pd.read_csv(os.path.join(causal_relationship_path, 'SKT_COPA_Dev.tsv'), delimiter='\\t')\n",
    "origin_copa_test_csv = pd.read_csv(os.path.join(causal_relationship_path, 'SKT_COPA_Test.tsv'), delimiter='\\t')\n",
    "\n",
    "print(origin_copa_train_csv['question'].unique())\n",
    "print(origin_copa_train_csv['Answer'].unique())\n",
    "print(origin_copa_dev_csv['question'].unique())\n",
    "print(origin_copa_dev_csv['Answer'].unique())\n",
    "print(origin_copa_test_csv['question'].unique())\n",
    "print(origin_copa_test_csv['Answer'].unique())\n",
    "\n",
    "def make_copa_csv(csv, phase='train'):\n",
    "    ids = csv['ID']\n",
    "    sentences = csv['sentence']\n",
    "    questions = csv['question']\n",
    "    candidate1 = csv['1']\n",
    "    candidate2 = csv['2']\n",
    "\n",
    "    rows = []\n",
    "    \n",
    "    if phase == 'train' or phase == 'dev':\n",
    "        answers = csv['Answer']\n",
    "        columns = ['ID', 'sentence', 'Answer']\n",
    "        \n",
    "        for i in range(len(questions)):\n",
    "            if questions[i].strip() == '결과':\n",
    "                if answers[i] == 1:\n",
    "                    rows.append([ids[i], sentences[i] + ' ' + candidate1[i], 1])\n",
    "                    rows.append([ids[i], sentences[i] + ' ' + candidate2[i], 0])\n",
    "                elif answers[i] == 2:\n",
    "                    rows.append([ids[i], sentences[i] + ' ' + candidate1[i], 0])\n",
    "                    rows.append([ids[i], sentences[i] + ' ' + candidate2[i], 1])\n",
    "                \n",
    "            elif questions[i].strip() == '원인':\n",
    "                if answers[i] == 1:\n",
    "                    rows.append([ids[i], candidate1[i] + ' ' + sentences[i], 1])\n",
    "                    rows.append([ids[i], candidate2[i] + ' ' + sentences[i], 0])\n",
    "                elif answers[i] == 2:\n",
    "                    rows.append([ids[i], candidate1[i] + ' ' + sentences[i], 0])\n",
    "                    rows.append([ids[i], candidate2[i] + ' ' + sentences[i], 1])\n",
    "        \n",
    "    elif phase == 'test':\n",
    "        columns = ['ID', 'sentence']\n",
    "        for i in range(len(questions)):\n",
    "            if questions[i].strip() == '결과':\n",
    "                rows.append([ids[i], sentences[i] + ' ' + candidate1[i]])\n",
    "                rows.append([ids[i], sentences[i] + ' ' + candidate2[i]])\n",
    "            elif questions[i].strip() == '원인':\n",
    "                rows.append([ids[i], candidate1[i] + ' ' + sentences[i]])\n",
    "                rows.append([ids[i], candidate2[i] + ' ' + sentences[i]])\n",
    "\n",
    "    dataset = pd.DataFrame(rows)\n",
    "    dataset.columns = columns\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "copa_train_csv = make_copa_csv(origin_copa_train_csv)\n",
    "copa_dev_csv = make_copa_csv(origin_copa_dev_csv, phase='dev')\n",
    "copa_test_csv = make_copa_csv(origin_copa_test_csv, phase='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcf2608a-8d97-4f78-b1b1-750c33fdeb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"skt/ko-gpt-trinity-1.2B-v0.5\")\n",
    "\n",
    "copa_train_encodings = tokenizer(list(copa_train_csv['sentence'].values), truncation=True, padding=True)\n",
    "copa_dev_encodings = tokenizer(list(copa_dev_csv['sentence'].values), truncation=True, padding=True)\n",
    "copa_test_encodings = tokenizer(list(copa_test_csv['sentence'].values), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15eda216-8574-42ab-beee-b562a48e0fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CopaDataset(Dataset):\n",
    "    def __init__(self, dataset, encodings, phase='train'):\n",
    "        self.copa_csv = dataset\n",
    "        self.encodings = encodings\n",
    "        self.phase = phase\n",
    "\n",
    "        self.ids = dataset['ID']\n",
    "        if self.phase == 'train' or self.phase == 'dev':\n",
    "            self.labels = dataset['Answer']\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.phase == 'train' or self.phase == 'dev':\n",
    "            item = {key: torch.tensor(val[index]) for key, val in self.encodings.items()}\n",
    "            item['labels'] = torch.tensor(self.labels[index])\n",
    "            return item\n",
    "        else:\n",
    "            return (self.ids[index], self.encodings[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.copa_csv['Answer'])\n",
    "\n",
    "copa_train_dataset = CopaDataset(copa_train_csv, copa_train_encodings)\n",
    "copa_dev_dataset = CopaDataset(copa_dev_csv, copa_dev_encodings, phase='dev')\n",
    "copa_test_dataset = CopaDataset(copa_test_csv, copa_test_encodings, phase='test')\n",
    "\n",
    "train_loader = DataLoader(copa_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = DataLoader(copa_dev_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(copa_test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ef19198-1760-4940-a520-4066f43edcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at skt/ko-gpt-trinity-1.2B-v0.5 were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at skt/ko-gpt-trinity-1.2B-v0.5 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'AdamW' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-27f35a2f9979>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'AdamW' is not defined"
     ]
    }
   ],
   "source": [
    "config = GPT2Config.from_pretrained(\"skt/ko-gpt-trinity-1.2B-v0.5\")\n",
    "config.num_labels = num_labels\n",
    "model = GPT2ForSequenceClassification(config).from_pretrained(\"skt/ko-gpt-trinity-1.2B-v0.5\")    \n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d41c17c2-e41c-4952-aef5-8d1ac0057ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, num_epochs, model, optimizer, scheduler=None):\n",
    "    model.train()\n",
    "\n",
    "    batch_loss_list = []\n",
    "    progress = ProgressMonitor(length=len(copa_train_dataset))\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss_list.append(loss.item())\n",
    "        progress.update(epoch, num_epochs, input_ids.shape[0], sum(batch_loss_list)/len(batch_loss_list))\n",
    "\n",
    "    if scheduler:\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7356da5c-6d78-4c04-a7e1-4b5e9ab8029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dev_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            outputs = torch.argmax(outputs.logits, 1)\n",
    "            correct += (outputs == labels).sum().item()\n",
    "\n",
    "    acc = 100 * float(correct) / len(copa_dev_dataset) \n",
    "    print('Test Acc: {}/{} ({:.2f}%)'.format(correct, len(copa_dev_dataset), acc))\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76115d8b-515b-4890-b285-e6ea2fe3665a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <table style=\"width: 100%;\">\n",
       "            <tbody>\n",
       "                <tr>\n",
       "                    <td style=\"width: 30%;\">\n",
       "                     <b>Epoch: 1/3 Loss: 0.7758</b> &nbsp&nbsp&nbsp 6160 / 6160\n",
       "                    </td>\n",
       "                    <td style=\"width: 70%;\">\n",
       "                        <progress value='6160' max='6160', style='width: 100%'>6160</progress>\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </tbody>\n",
       "        </table>        \n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 500/1000 (50.00%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <table style=\"width: 100%;\">\n",
       "            <tbody>\n",
       "                <tr>\n",
       "                    <td style=\"width: 30%;\">\n",
       "                     <b>Epoch: 2/3 Loss: 0.7237</b> &nbsp&nbsp&nbsp 6160 / 6160\n",
       "                    </td>\n",
       "                    <td style=\"width: 70%;\">\n",
       "                        <progress value='6160' max='6160', style='width: 100%'>6160</progress>\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </tbody>\n",
       "        </table>        \n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 500/1000 (50.00%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <table style=\"width: 100%;\">\n",
       "            <tbody>\n",
       "                <tr>\n",
       "                    <td style=\"width: 30%;\">\n",
       "                     <b>Epoch: 3/3 Loss: 0.7188</b> &nbsp&nbsp&nbsp 6160 / 6160\n",
       "                    </td>\n",
       "                    <td style=\"width: 70%;\">\n",
       "                        <progress value='6160' max='6160', style='width: 100%'>6160</progress>\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </tbody>\n",
       "        </table>        \n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 517/1000 (51.70%)\n",
      "Training completed in 8m 10s\n",
      "Best test accuracy: 51.700000\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "\n",
    "best_model_weights = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train(epoch+1, num_epochs, model, optim)\n",
    "    acc = validate(model)\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_model_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "model.load_state_dict( best_model_weights )\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('Training completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "print('Best test accuracy: {:4f}'.format(best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f95806-e3b1-4a5b-b63d-7606b56bf13c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d960e8-8d98-45c3-9799-85bc0e1bb7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472cab11-0b64-4436-a81b-edcd1694a5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b32fa5-e424-45ef-a8ff-abd085df0f61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
